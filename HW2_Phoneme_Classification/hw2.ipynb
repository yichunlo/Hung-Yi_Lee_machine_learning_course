{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「SHARE_MLSpring2021_HW2_1_ipynb」的副本 (5).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, \n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "e352a238-08f0-45c7-eee2-c9cd5b51e445"
      },
      "source": [
        "#!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "a001c414-e11c-4df3-dbd3-39c6d6b8b843"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./drive/MyDrive/timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYqi_lAuvC59",
        "outputId": "302485d0-0719-4d5b-966e-22a58d6c13e4"
      },
      "source": [
        "VAL_RATIO = 0.1\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (1106938, 429)\n",
            "Size of validation set: (122994, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUCbQvqJurYc"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "f115b673-6cd2-4751-886b-8da49658f904"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            #nn.Conv1d()\n",
        "            nn.Linear(429, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 1600),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1600),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1600, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2048, 1600),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1600),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1600, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 39),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTp3ZXg1yO9Y",
        "outputId": "c6066284-fb20-4c72-bb1a-204e08e0abb0"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(3)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 140           # number of training epoch\n",
        "learning_rate = 0.000035       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMWsBs7zzNs",
        "outputId": "19865852-2335-49ce-f08b-ff7f92abb8ac"
      },
      "source": [
        "# start training\n",
        "candidate_num = 1\n",
        "for idx in range(candidate_num):\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epoch):\n",
        "        train_acc = 0.0\n",
        "        train_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "        val_loss = 0.0\n",
        "\n",
        "        # training\n",
        "        model.train() # set the model to training mode\n",
        "        for i, data in enumerate(train_loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad() \n",
        "            outputs = model(inputs) \n",
        "            batch_loss = criterion(outputs, labels)\n",
        "            _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "            batch_loss.backward() \n",
        "            optimizer.step() \n",
        "\n",
        "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "            train_loss += batch_loss.item()\n",
        "\n",
        "        # validation\n",
        "        if len(val_set) > 0:\n",
        "            model.eval() # set the model to evaluation mode\n",
        "            with torch.no_grad():\n",
        "                for i, data in enumerate(val_loader):\n",
        "                    inputs, labels = data\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    batch_loss = criterion(outputs, labels) \n",
        "                    _, val_pred = torch.max(outputs, 1) \n",
        "                \n",
        "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                    val_loss += batch_loss.item()\n",
        "\n",
        "                print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                    epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "                ))\n",
        "\n",
        "                # if the model improves, save a checkpoint at this epoch\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    torch.save(model.state_dict(), model_path)\n",
        "                    print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "        else:\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "            ))\n",
        "\n",
        "    # if not validating, save the last epoch\n",
        "    if len(val_set) == 0:\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print('saving model at last epoch')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/140] Train Acc: 0.448304 Loss: 1.949598 | Val Acc: 0.593631 loss: 1.340552\n",
            "saving model with acc 0.594\n",
            "[002/140] Train Acc: 0.570282 Loss: 1.420982 | Val Acc: 0.647771 loss: 1.133974\n",
            "saving model with acc 0.648\n",
            "[003/140] Train Acc: 0.605196 Loss: 1.282857 | Val Acc: 0.670334 loss: 1.044028\n",
            "saving model with acc 0.670\n",
            "[004/140] Train Acc: 0.624619 Loss: 1.207109 | Val Acc: 0.684895 loss: 0.990438\n",
            "saving model with acc 0.685\n",
            "[005/140] Train Acc: 0.637825 Loss: 1.157230 | Val Acc: 0.696506 loss: 0.947717\n",
            "saving model with acc 0.697\n",
            "[006/140] Train Acc: 0.648551 Loss: 1.118079 | Val Acc: 0.701815 loss: 0.919793\n",
            "saving model with acc 0.702\n",
            "[007/140] Train Acc: 0.656461 Loss: 1.086444 | Val Acc: 0.709896 loss: 0.896978\n",
            "saving model with acc 0.710\n",
            "[008/140] Train Acc: 0.663498 Loss: 1.060788 | Val Acc: 0.715921 loss: 0.876328\n",
            "saving model with acc 0.716\n",
            "[009/140] Train Acc: 0.669198 Loss: 1.039437 | Val Acc: 0.718246 loss: 0.863429\n",
            "saving model with acc 0.718\n",
            "[010/140] Train Acc: 0.674611 Loss: 1.020211 | Val Acc: 0.721970 loss: 0.851385\n",
            "saving model with acc 0.722\n",
            "[011/140] Train Acc: 0.679018 Loss: 1.003446 | Val Acc: 0.725735 loss: 0.837790\n",
            "saving model with acc 0.726\n",
            "[012/140] Train Acc: 0.682340 Loss: 0.988390 | Val Acc: 0.727670 loss: 0.831001\n",
            "saving model with acc 0.728\n",
            "[013/140] Train Acc: 0.686178 Loss: 0.975429 | Val Acc: 0.730987 loss: 0.820497\n",
            "saving model with acc 0.731\n",
            "[014/140] Train Acc: 0.689651 Loss: 0.961771 | Val Acc: 0.732296 loss: 0.811417\n",
            "saving model with acc 0.732\n",
            "[015/140] Train Acc: 0.692375 Loss: 0.951914 | Val Acc: 0.735231 loss: 0.806287\n",
            "saving model with acc 0.735\n",
            "[016/140] Train Acc: 0.695263 Loss: 0.941164 | Val Acc: 0.736621 loss: 0.799411\n",
            "saving model with acc 0.737\n",
            "[017/140] Train Acc: 0.698169 Loss: 0.931110 | Val Acc: 0.739191 loss: 0.792277\n",
            "saving model with acc 0.739\n",
            "[018/140] Train Acc: 0.700591 Loss: 0.922503 | Val Acc: 0.739987 loss: 0.786792\n",
            "saving model with acc 0.740\n",
            "[019/140] Train Acc: 0.703151 Loss: 0.914395 | Val Acc: 0.740995 loss: 0.782998\n",
            "saving model with acc 0.741\n",
            "[020/140] Train Acc: 0.705629 Loss: 0.905721 | Val Acc: 0.743581 loss: 0.776195\n",
            "saving model with acc 0.744\n",
            "[021/140] Train Acc: 0.707171 Loss: 0.897649 | Val Acc: 0.745841 loss: 0.771814\n",
            "saving model with acc 0.746\n",
            "[022/140] Train Acc: 0.709611 Loss: 0.889156 | Val Acc: 0.745191 loss: 0.770804\n",
            "[023/140] Train Acc: 0.711755 Loss: 0.882296 | Val Acc: 0.746817 loss: 0.764516\n",
            "saving model with acc 0.747\n",
            "[024/140] Train Acc: 0.713039 Loss: 0.877364 | Val Acc: 0.747841 loss: 0.761153\n",
            "saving model with acc 0.748\n",
            "[025/140] Train Acc: 0.715001 Loss: 0.869641 | Val Acc: 0.748451 loss: 0.758259\n",
            "saving model with acc 0.748\n",
            "[026/140] Train Acc: 0.717020 Loss: 0.863858 | Val Acc: 0.750443 loss: 0.755891\n",
            "saving model with acc 0.750\n",
            "[027/140] Train Acc: 0.718750 Loss: 0.858467 | Val Acc: 0.750028 loss: 0.752817\n",
            "[028/140] Train Acc: 0.719936 Loss: 0.853491 | Val Acc: 0.751598 loss: 0.749344\n",
            "saving model with acc 0.752\n",
            "[029/140] Train Acc: 0.721619 Loss: 0.846679 | Val Acc: 0.753630 loss: 0.745372\n",
            "saving model with acc 0.754\n",
            "[030/140] Train Acc: 0.723151 Loss: 0.841799 | Val Acc: 0.753004 loss: 0.746111\n",
            "[031/140] Train Acc: 0.724817 Loss: 0.836230 | Val Acc: 0.754411 loss: 0.744110\n",
            "saving model with acc 0.754\n",
            "[032/140] Train Acc: 0.726083 Loss: 0.831145 | Val Acc: 0.753329 loss: 0.742544\n",
            "[033/140] Train Acc: 0.727709 Loss: 0.826295 | Val Acc: 0.753500 loss: 0.740097\n",
            "[034/140] Train Acc: 0.728635 Loss: 0.821497 | Val Acc: 0.755769 loss: 0.735767\n",
            "saving model with acc 0.756\n",
            "[035/140] Train Acc: 0.730504 Loss: 0.816831 | Val Acc: 0.755094 loss: 0.736477\n",
            "[036/140] Train Acc: 0.731187 Loss: 0.812408 | Val Acc: 0.756191 loss: 0.732192\n",
            "saving model with acc 0.756\n",
            "[037/140] Train Acc: 0.732648 Loss: 0.808116 | Val Acc: 0.757069 loss: 0.729122\n",
            "saving model with acc 0.757\n",
            "[038/140] Train Acc: 0.733498 Loss: 0.804473 | Val Acc: 0.757435 loss: 0.727983\n",
            "saving model with acc 0.757\n",
            "[039/140] Train Acc: 0.734527 Loss: 0.800827 | Val Acc: 0.758362 loss: 0.726690\n",
            "saving model with acc 0.758\n",
            "[040/140] Train Acc: 0.736132 Loss: 0.795710 | Val Acc: 0.757826 loss: 0.726478\n",
            "[041/140] Train Acc: 0.737335 Loss: 0.792502 | Val Acc: 0.759379 loss: 0.724871\n",
            "saving model with acc 0.759\n",
            "[042/140] Train Acc: 0.738260 Loss: 0.788881 | Val Acc: 0.759005 loss: 0.724447\n",
            "[043/140] Train Acc: 0.740032 Loss: 0.783412 | Val Acc: 0.760240 loss: 0.719530\n",
            "saving model with acc 0.760\n",
            "[044/140] Train Acc: 0.739630 Loss: 0.782281 | Val Acc: 0.760663 loss: 0.720974\n",
            "saving model with acc 0.761\n",
            "[045/140] Train Acc: 0.741476 Loss: 0.776705 | Val Acc: 0.760110 loss: 0.722477\n",
            "[046/140] Train Acc: 0.742648 Loss: 0.774214 | Val Acc: 0.759013 loss: 0.720278\n",
            "[047/140] Train Acc: 0.743328 Loss: 0.770058 | Val Acc: 0.761183 loss: 0.717171\n",
            "saving model with acc 0.761\n",
            "[048/140] Train Acc: 0.743728 Loss: 0.768032 | Val Acc: 0.761509 loss: 0.718137\n",
            "saving model with acc 0.762\n",
            "[049/140] Train Acc: 0.745181 Loss: 0.764089 | Val Acc: 0.762590 loss: 0.716938\n",
            "saving model with acc 0.763\n",
            "[050/140] Train Acc: 0.745710 Loss: 0.761666 | Val Acc: 0.762907 loss: 0.713209\n",
            "saving model with acc 0.763\n",
            "[051/140] Train Acc: 0.747094 Loss: 0.757954 | Val Acc: 0.762671 loss: 0.717412\n",
            "[052/140] Train Acc: 0.747408 Loss: 0.756116 | Val Acc: 0.762590 loss: 0.716486\n",
            "[053/140] Train Acc: 0.748494 Loss: 0.751495 | Val Acc: 0.762143 loss: 0.715586\n",
            "[054/140] Train Acc: 0.749707 Loss: 0.749432 | Val Acc: 0.762289 loss: 0.715134\n",
            "[055/140] Train Acc: 0.750063 Loss: 0.746151 | Val Acc: 0.764029 loss: 0.711829\n",
            "saving model with acc 0.764\n",
            "[056/140] Train Acc: 0.750463 Loss: 0.744694 | Val Acc: 0.764663 loss: 0.712136\n",
            "saving model with acc 0.765\n",
            "[057/140] Train Acc: 0.751693 Loss: 0.741763 | Val Acc: 0.764200 loss: 0.711473\n",
            "[058/140] Train Acc: 0.752266 Loss: 0.738280 | Val Acc: 0.764216 loss: 0.709834\n",
            "[059/140] Train Acc: 0.753214 Loss: 0.735953 | Val Acc: 0.764533 loss: 0.711042\n",
            "[060/140] Train Acc: 0.753917 Loss: 0.733274 | Val Acc: 0.765558 loss: 0.708600\n",
            "saving model with acc 0.766\n",
            "[061/140] Train Acc: 0.753942 Loss: 0.731696 | Val Acc: 0.765541 loss: 0.708135\n",
            "[062/140] Train Acc: 0.755441 Loss: 0.727629 | Val Acc: 0.764428 loss: 0.709357\n",
            "[063/140] Train Acc: 0.755653 Loss: 0.726931 | Val Acc: 0.765419 loss: 0.710421\n",
            "[064/140] Train Acc: 0.756910 Loss: 0.724635 | Val Acc: 0.765997 loss: 0.707356\n",
            "saving model with acc 0.766\n",
            "[065/140] Train Acc: 0.757332 Loss: 0.721974 | Val Acc: 0.765712 loss: 0.706290\n",
            "[066/140] Train Acc: 0.757908 Loss: 0.720148 | Val Acc: 0.765680 loss: 0.708682\n",
            "[067/140] Train Acc: 0.758181 Loss: 0.718072 | Val Acc: 0.766997 loss: 0.707045\n",
            "saving model with acc 0.767\n",
            "[068/140] Train Acc: 0.758898 Loss: 0.715200 | Val Acc: 0.766558 loss: 0.705193\n",
            "[069/140] Train Acc: 0.759958 Loss: 0.713307 | Val Acc: 0.766924 loss: 0.705827\n",
            "[070/140] Train Acc: 0.760082 Loss: 0.712188 | Val Acc: 0.766208 loss: 0.705119\n",
            "[071/140] Train Acc: 0.760797 Loss: 0.708768 | Val Acc: 0.767151 loss: 0.704329\n",
            "saving model with acc 0.767\n",
            "[072/140] Train Acc: 0.761175 Loss: 0.707860 | Val Acc: 0.767233 loss: 0.706274\n",
            "saving model with acc 0.767\n",
            "[073/140] Train Acc: 0.761437 Loss: 0.706206 | Val Acc: 0.767159 loss: 0.705170\n",
            "[074/140] Train Acc: 0.762093 Loss: 0.703818 | Val Acc: 0.766655 loss: 0.704602\n",
            "[075/140] Train Acc: 0.762536 Loss: 0.702346 | Val Acc: 0.768029 loss: 0.703923\n",
            "saving model with acc 0.768\n",
            "[076/140] Train Acc: 0.763355 Loss: 0.700419 | Val Acc: 0.768314 loss: 0.702541\n",
            "saving model with acc 0.768\n",
            "[077/140] Train Acc: 0.764272 Loss: 0.697889 | Val Acc: 0.767956 loss: 0.702135\n",
            "[078/140] Train Acc: 0.764298 Loss: 0.696286 | Val Acc: 0.768005 loss: 0.703924\n",
            "[079/140] Train Acc: 0.764383 Loss: 0.695234 | Val Acc: 0.767802 loss: 0.701234\n",
            "[080/140] Train Acc: 0.765990 Loss: 0.692312 | Val Acc: 0.767948 loss: 0.703988\n",
            "[081/140] Train Acc: 0.766304 Loss: 0.690201 | Val Acc: 0.768826 loss: 0.704444\n",
            "saving model with acc 0.769\n",
            "[082/140] Train Acc: 0.766542 Loss: 0.689335 | Val Acc: 0.768769 loss: 0.703756\n",
            "[083/140] Train Acc: 0.767064 Loss: 0.687407 | Val Acc: 0.769420 loss: 0.702830\n",
            "saving model with acc 0.769\n",
            "[084/140] Train Acc: 0.767344 Loss: 0.686777 | Val Acc: 0.768460 loss: 0.704020\n",
            "[085/140] Train Acc: 0.768132 Loss: 0.683970 | Val Acc: 0.769680 loss: 0.700683\n",
            "saving model with acc 0.770\n",
            "[086/140] Train Acc: 0.769075 Loss: 0.682040 | Val Acc: 0.769387 loss: 0.702697\n",
            "[087/140] Train Acc: 0.768822 Loss: 0.681581 | Val Acc: 0.769720 loss: 0.701687\n",
            "saving model with acc 0.770\n",
            "[088/140] Train Acc: 0.769140 Loss: 0.680066 | Val Acc: 0.769216 loss: 0.700280\n",
            "[089/140] Train Acc: 0.769600 Loss: 0.679575 | Val Acc: 0.769574 loss: 0.700724\n",
            "[090/140] Train Acc: 0.770165 Loss: 0.676879 | Val Acc: 0.770477 loss: 0.700587\n",
            "saving model with acc 0.770\n",
            "[091/140] Train Acc: 0.770135 Loss: 0.675846 | Val Acc: 0.770908 loss: 0.701529\n",
            "saving model with acc 0.771\n",
            "[092/140] Train Acc: 0.770961 Loss: 0.674004 | Val Acc: 0.770452 loss: 0.702240\n",
            "[093/140] Train Acc: 0.771737 Loss: 0.671657 | Val Acc: 0.770501 loss: 0.702111\n",
            "[094/140] Train Acc: 0.771608 Loss: 0.671487 | Val Acc: 0.769891 loss: 0.703067\n",
            "[095/140] Train Acc: 0.772477 Loss: 0.670213 | Val Acc: 0.770639 loss: 0.703196\n",
            "[096/140] Train Acc: 0.772913 Loss: 0.667904 | Val Acc: 0.770810 loss: 0.700560\n",
            "[097/140] Train Acc: 0.773571 Loss: 0.665432 | Val Acc: 0.771013 loss: 0.702410\n",
            "saving model with acc 0.771\n",
            "[098/140] Train Acc: 0.773520 Loss: 0.666074 | Val Acc: 0.769696 loss: 0.701006\n",
            "[099/140] Train Acc: 0.773995 Loss: 0.663944 | Val Acc: 0.770664 loss: 0.700024\n",
            "[100/140] Train Acc: 0.773779 Loss: 0.663893 | Val Acc: 0.771590 loss: 0.699910\n",
            "saving model with acc 0.772\n",
            "[101/140] Train Acc: 0.774680 Loss: 0.660840 | Val Acc: 0.771273 loss: 0.699768\n",
            "[102/140] Train Acc: 0.774186 Loss: 0.661375 | Val Acc: 0.771818 loss: 0.699557\n",
            "saving model with acc 0.772\n",
            "[103/140] Train Acc: 0.775511 Loss: 0.659125 | Val Acc: 0.771997 loss: 0.699378\n",
            "saving model with acc 0.772\n",
            "[104/140] Train Acc: 0.775360 Loss: 0.658995 | Val Acc: 0.771777 loss: 0.700938\n",
            "[105/140] Train Acc: 0.775991 Loss: 0.657505 | Val Acc: 0.771509 loss: 0.701110\n",
            "[106/140] Train Acc: 0.776203 Loss: 0.655724 | Val Acc: 0.770745 loss: 0.702409\n",
            "[107/140] Train Acc: 0.776861 Loss: 0.654040 | Val Acc: 0.771298 loss: 0.700928\n",
            "[108/140] Train Acc: 0.776726 Loss: 0.654242 | Val Acc: 0.770631 loss: 0.700950\n",
            "[109/140] Train Acc: 0.777596 Loss: 0.652093 | Val Acc: 0.771070 loss: 0.703693\n",
            "[110/140] Train Acc: 0.777556 Loss: 0.651458 | Val Acc: 0.772208 loss: 0.701191\n",
            "saving model with acc 0.772\n",
            "[111/140] Train Acc: 0.777932 Loss: 0.649797 | Val Acc: 0.771412 loss: 0.699806\n",
            "[112/140] Train Acc: 0.778234 Loss: 0.648558 | Val Acc: 0.771176 loss: 0.700703\n",
            "[113/140] Train Acc: 0.778474 Loss: 0.648697 | Val Acc: 0.771111 loss: 0.703271\n",
            "[114/140] Train Acc: 0.778752 Loss: 0.646702 | Val Acc: 0.771550 loss: 0.701151\n",
            "[115/140] Train Acc: 0.779000 Loss: 0.645853 | Val Acc: 0.771981 loss: 0.703614\n",
            "[116/140] Train Acc: 0.779444 Loss: 0.644846 | Val Acc: 0.771574 loss: 0.701434\n",
            "[117/140] Train Acc: 0.779715 Loss: 0.644415 | Val Acc: 0.772542 loss: 0.699178\n",
            "saving model with acc 0.773\n",
            "[118/140] Train Acc: 0.780577 Loss: 0.641800 | Val Acc: 0.772005 loss: 0.700455\n",
            "[119/140] Train Acc: 0.780897 Loss: 0.641088 | Val Acc: 0.771501 loss: 0.702037\n",
            "[120/140] Train Acc: 0.780541 Loss: 0.641229 | Val Acc: 0.771753 loss: 0.701617\n",
            "[121/140] Train Acc: 0.781108 Loss: 0.639657 | Val Acc: 0.772656 loss: 0.699325\n",
            "saving model with acc 0.773\n",
            "[122/140] Train Acc: 0.781342 Loss: 0.639281 | Val Acc: 0.771908 loss: 0.701528\n",
            "[123/140] Train Acc: 0.781235 Loss: 0.638378 | Val Acc: 0.772298 loss: 0.701744\n",
            "[124/140] Train Acc: 0.781432 Loss: 0.637557 | Val Acc: 0.773574 loss: 0.701461\n",
            "saving model with acc 0.774\n",
            "[125/140] Train Acc: 0.782196 Loss: 0.635127 | Val Acc: 0.772111 loss: 0.704498\n",
            "[126/140] Train Acc: 0.782409 Loss: 0.634864 | Val Acc: 0.771428 loss: 0.701957\n",
            "[127/140] Train Acc: 0.783241 Loss: 0.633257 | Val Acc: 0.772891 loss: 0.701502\n",
            "[128/140] Train Acc: 0.783016 Loss: 0.633553 | Val Acc: 0.772282 loss: 0.702214\n",
            "[129/140] Train Acc: 0.783452 Loss: 0.632773 | Val Acc: 0.772420 loss: 0.701954\n",
            "[130/140] Train Acc: 0.783717 Loss: 0.631188 | Val Acc: 0.772436 loss: 0.703866\n",
            "[131/140] Train Acc: 0.783814 Loss: 0.631150 | Val Acc: 0.773314 loss: 0.700208\n",
            "[132/140] Train Acc: 0.783690 Loss: 0.629254 | Val Acc: 0.772656 loss: 0.703780\n",
            "[133/140] Train Acc: 0.784023 Loss: 0.628907 | Val Acc: 0.772989 loss: 0.702214\n",
            "[134/140] Train Acc: 0.784805 Loss: 0.627455 | Val Acc: 0.772306 loss: 0.701760\n",
            "[135/140] Train Acc: 0.784822 Loss: 0.627437 | Val Acc: 0.773387 loss: 0.700876\n",
            "[136/140] Train Acc: 0.784924 Loss: 0.626409 | Val Acc: 0.771525 loss: 0.704814\n",
            "[137/140] Train Acc: 0.785593 Loss: 0.625715 | Val Acc: 0.772160 loss: 0.703795\n",
            "[138/140] Train Acc: 0.785619 Loss: 0.624745 | Val Acc: 0.772525 loss: 0.701904\n",
            "[139/140] Train Acc: 0.786030 Loss: 0.623751 | Val Acc: 0.773192 loss: 0.700229\n",
            "[140/140] Train Acc: 0.786157 Loss: 0.622550 | Val Acc: 0.772647 loss: 0.701464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PKjtAScPWtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce88462c-531a-4103-8b3b-93f8db65f5f5"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}